# Extracting Information from Electrode and Neural Spike Recordings from Macaque Monkeys
## With Kevin Lin and Alexa Aucoin, Summer 2021
How do we determine the type of stimulus administered to a monkey given only access to their neural recordings? How do we extract the patterns associated with each stimulus type? How do we choose the most important electrodes and neurons in producing these patterns? All of these were questions we asked and attempted to answer in this summer project. 

In the dataset we worked with, the monkeys were given several types of stimulus while deep brain electrodes were recording from two seperate brain regions (denoted by A and B due to the data being not yet published). While brain region A responded very strongly to the different types of stimulus, the other did not appear to. What we attempted to do was connect the activity of the two brain regions, and extract a potentially complex signal from region B. We obtained a preliminary finding that the two brain regions may be encoding nearly independant information. However, we werent able to entirely flesh it out by the time we wrapped up the project. 

In addition, we investigated the performance of different feature selection algorithms on the electrode and neuron recordings over both linear Support Vector Machines and Nearest Centroid with the Mahalnobis Distance classifiers. One method we found particular interesting was the commonly used leverage based feature selector and its performance in comparison to Principal Component Analysis (PCA). The idea of leverage stems from statistical regression techniques. A high leverage point in regression is a point that has a high degree of distance away from other points in the independant variables. Because of this, it has the ability to influence, or pull, the regression line towards itself. Leverage in the context of PCA is very similar except that instead of a row of data pulling a regression line, a data column pulls the first k left eigenvectors of the dataset towards itself. A leverage based feature selector calculates the leverage of each data column, and then selects the columns with the highest leverage. Presumably, these vectors then construct most of the top k eigenvectors and as such they should capture the most important data columns. In our datasets however, we noticed a discrepency in how well the leverage based feature selector performed in comparison to the top k left eigenvectors on linear SVMs. The performance didn't seem to match between them, with the eigenvectors dominating the leverage vectors by a large margin. So, we dug into this analytically, and built a new algorithm that, while more computationally expensive than leverage, actually constructs a subspace that is nearer to the left k eigenvectors and performs more similarly. 

In addition, we found that sometimes an extremely simple feature selector can outperform anything fancy. Over a small label class size, we selected data vectors that had the highest difference in pairwise means between the other labels. This basic feature selector outperformed both mutual information and PCA based feature selectors. 

In this figure we can see both of these results, the algorithm we built (here called subspace) oscillates in performance around PCA, and the pairwise mean seperation algorithm (mean) solidly outperforms the rest. 

<img src = "./images/Monkeys.png" alt="responsive" style="border:solid #292b2c;width:65vw;height:auto"/>